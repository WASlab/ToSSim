#!/usr/bin/env bash
# =======================================================================
#  scratch‑only vLLM install (Python 3.11 ▸ CUDA 12.6 ▸ nightly wheels)
# =======================================================================
set -euo pipefail
trap 'echo "[!] Aborted on line $LINENO"; exit 1' ERR

# -----------------------------------------------------------------------
# 0️⃣  Scratch + HF cache setup
# -----------------------------------------------------------------------
SCRATCH=${SCRATCH:-"$HOME/scratch"}
[[ -w "$SCRATCH" ]] || { echo "❌  $SCRATCH not writable"; exit 1; }
echo "[✓] Scratch dir: $SCRATCH"

export HF_HOME="$SCRATCH/hf_cache"
for d in transformers datasets metrics hub; do mkdir -p "$HF_HOME/$d"; done
cd "$SCRATCH"
TOSSIM="$SCRATCH/ToSSim"
if [[ ! -d "$TOSSIM/.git" || -z "$(ls -A "$TOSSIM")" ]]; then
  echo "[+] Cloning Salem-AI-Sim/ToSSim → $TOSSIM"
  rm -rf "$TOSSIM"
  git clone --depth 1 https://github.com/Salem-AI-Sim/ToSSim.git "$TOSSIM"
else
  echo "[+] ToSSim already present; updating (git pull)"
  git -C "$TOSSIM" pull origin main
fi

# -----------------------------------------------------------------------
# 1️⃣  Miniforge bootstrap  (re‑use if present)
# -----------------------------------------------------------------------
MF="$SCRATCH/miniforge3"

if ! command -v conda &>/dev/null; then          # conda not on PATH
  if [[ -d "$MF" ]]; then                        # directory already there
    echo "[+] Found existing Miniforge at $MF – re‑using it"
  else
    echo "[+] Installing Miniforge → $MF"
    curl -sL https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh -o miniforge.sh
    bash miniforge.sh -b -p "$MF"
    rm miniforge.sh
  fi
  export PATH="$MF/bin:$PATH"                    # make conda visible
fi
source "$MF/etc/profile.d/conda.sh"


# -----------------------------------------------------------------------
# 2️⃣  Python‑3.11 env
# -----------------------------------------------------------------------
ENV="$SCRATCH/vllm311"
[[ -d "$ENV" ]] || conda create -y -p "$ENV" python=3.11
conda activate "$ENV"

# -----------------------------------------------------------------------
# 3️⃣  Core deps + vLLM
# -----------------------------------------------------------------------
python -m pip install -U pip setuptools wheel
python -m pip install jinja2   # torch dep on some distros
python -m pip install pytest
# ---- 3a. pick a vLLM wheel if available --------------------------------
VLLM_VERSION="${VLLM_VERSION:-"nightly"}"    # export to pin a tag
echo "[i] Desired vllm version: $VLLM_VERSION"

if [[ "$VLLM_VERSION" == "nightly" ]]; then
  PIP_SPEC="vllm"
  PIP_EXTRA="--extra-index-url https://wheels.vllm.ai/nightly"
else
  PIP_SPEC="vllm==$VLLM_VERSION"
  PIP_EXTRA=""
fi

if ! python -m pip install --pre --only-binary=:all: $PIP_EXTRA $PIP_SPEC ; then
  echo "[!] Wheel not found – compiling from source"
  SRC="$SCRATCH/vllm_src"
  rm -rf "$SRC"          # wipe any previous half‑clone
  git clone --depth 1 https://github.com/vllm-project/vllm.git "$SRC"
  python -m pip install -v -e "$SRC[cuda12]"
fi
FLASH_WHEEL="https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.10/flash_attn-2.7.4+cu128torch2.7-cp311-cp311-linux_x86_64.whl"
echo "[+] Installing Flash‑Attention …"
python -m pip install "$FLASH_WHEEL" --no-cache-dir || {
  echo "[!] Wheel mismatch → building from source (slow) …"
  python -m pip install --no-build-isolation flash-attn
}
# ---- 3b. helpers --------------------------------------------------------
python -m pip install transformers sentencepiece tokenizers protobuf pandas fire bitsandbytes accelerate
pip install -U hf_transfer
pip install liger-kernel-nightly
# -----------------------------------------------------------------------
# 4️⃣  Summary banner
# -----------------------------------------------------------------------
python - <<'PY'
import torch, pkg_resources, os, platform, sys
print("\n✅  vLLM env ready")
print(" • Python :", sys.version.split()[0])
print(" • Torch  :", torch.__version__, "| CUDA:", torch.version.cuda)
print(" • vLLM   :", pkg_resources.get_distribution('vllm').version)
print(" • HF cache:", os.environ['HF_HOME'])
print(" • Host   :", platform.node())
PY
echo "---------------------------------------------------------"
echo "Re‑enter later with:  conda activate $ENV"

# -----------------------------------------------------------------------
# 5️⃣  Debug‑friendly env‑vars (optional)
# -----------------------------------------------------------------------
export VLLM_DISABLE_MULTIMODAL=1
export VLLM_USE_MULTIMODAL=0
export VLLM_USE_V1=0           # eager‑only, avoids CUDA‑graph issues
export VLLM_ENFORCE_EAGER=1
export TOSSIM_STREAM=1
export HF_HOME=/home/hice1/wstigall6/scratch/hf_cache
export TRANSFORMERS_CACHE=$HF_HOME/hub
export HF_DATASETS_CACHE=$HF_HOME/datasets
export HF_HOME=/storage/ice1/3/7/wstigall6/hf_cache
export TRANSFORMERS_CACHE=$HF_HOME/hub            # legacy var still respected
export HF_DATASETS_CACHE=$HF_HOME/datasets
export XDG_CACHE_HOME=/storage/ice1/3/7/wstigall6/xdg_cache  # vLLM reads this
mkdir -p "$TRANSFORMERS_CACHE" "$HF_DATASETS_CACHE" "$XDG_CACHE_HOME"
export CACHE_ROOT=/storage/ice1/3/7/wstigall6/cache

# HF / Transformers
export HF_HOME=$CACHE_ROOT/hf
export TRANSFORMERS_CACHE=$HF_HOME/hub          # legacy still read
export HF_DATASETS_CACHE=$HF_HOME/datasets
export HF_HUB_UPLOAD_CHUNK_SIZE=16777216
export HF_HUB_ENABLE_HF_TRANSFER=1  # if you installed hf_transfer

# vLLM & generic caches via XDG
export XDG_CACHE_HOME=$CACHE_ROOT/xdg

# PyTorch / Triton / TorchInductor
export TORCH_HOME=$CACHE_ROOT/torch
export TORCH_EXTENSIONS_DIR=$CACHE_ROOT/torch_extensions
export TORCHINDUCTOR_CACHE_DIR=$CACHE_ROOT/torchinductor
export TRITON_CACHE_DIR=$CACHE_ROOT/triton

# CUDA & numba
export CUDA_CACHE_PATH=$CACHE_ROOT/cuda
export NUMBA_CACHE_DIR=$CACHE_ROOT/numba

# pip
export PIP_CACHE_DIR=$CACHE_ROOT/pip

# tmp (avoid /home and avoid node-local /tmp if it evaporates between jobs)
export TMPDIR=$CACHE_ROOT/tmp

mkdir -p \
  "$HF_HOME" "$TRANSFORMERS_CACHE" "$HF_DATASETS_CACHE" \
  "$XDG_CACHE_HOME" "$TORCH_EXTENSIONS_DIR" "$TORCHINDUCTOR_CACHE_DIR" \
  "$TRITON_CACHE_DIR" "$CUDA_CACHE_PATH" "$NUMBA_CACHE_DIR" \
  "$PIP_CACHE_DIR" "$TMPDIR"
# Optional: pip cache too
export PIP_CACHE_DIR=/storage/ice1/3/7/wstigall6/pip_cache

cd ToSSim
# =======================================================================
