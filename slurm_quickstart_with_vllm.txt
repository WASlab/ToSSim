# ====================  FULL SCRATCH‑ONLY vLLM SETUP  =====================
set -e
trap 'echo "[!] Script aborted on line $LINENO"; exit 1' ERR

# 0️⃣  Resolve scratch and Hugging‑Face caches
SCRATCH=${SCRATCH:-$HOME/scratch}
[[ -d "$SCRATCH" && -w "$SCRATCH" ]] || { echo "❌  $SCRATCH not writable."; exit 1; }
echo "[✓] Scratch: $SCRATCH"

export HF_HOME="$SCRATCH/hf_cache"
export TRANSFORMERS_CACHE="$HF_HOME/transformers"
export HF_DATASETS_CACHE="$HF_HOME/datasets"
export HF_HUB_CACHE="$HF_HOME/hub"

cd "$SCRATCH"

# 1️⃣  Ensure Miniforge (reuse if present)
MF="$SCRATCH/miniforge3"
if ! command -v conda &>/dev/null; then
  echo "[+] Miniforge not on PATH → installing to $MF"
  [[ -d "$MF" ]] || {
    curl -sL https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh -o "$SCRATCH/miniforge.sh"
    bash "$SCRATCH/miniforge.sh" -b -p "$MF"
    rm "$SCRATCH/miniforge.sh"
  }
  export PATH="$MF/bin:$PATH"
else
  echo "[✓] Conda already on PATH"
fi
source "$(conda info --base)/etc/profile.d/conda.sh"

# 2️⃣  Create / activate **vllm311** env
ENV="$SCRATCH/vllm311"
if [[ -d "$ENV" ]]; then
  echo "[+] vLLM env exists → activating $ENV"
else
  echo "[+] Creating Python‑3.11 vLLM env at $ENV"
  conda create -y -p "$ENV" python=3.11
fi
conda activate "$ENV"
echo "[✓] Env  : $ENV  (Python $(python -V | awk '{print $2}'))"

# 3️⃣  Install torch + vLLM stack (CUDA 11/12 wheels auto‑picked by vLLM)
python -m pip install -U pip setuptools wheel
python -m pip install "torch>=2.2"                \
                      "vllm[cuda12]"              \
                      transformers                \
                      sentencepiece               \
                      tokenizers                  \
                      protobuf\
                      pandas

# 4️⃣  Summary banner
python - <<'PY'
import torch, platform, sys, pkg_resources, os
print("✅  vLLM scratch‑only inference env ready!")
print(" • Python :", sys.version.split()[0])
print(" • Torch  :", torch.__version__, "| CUDA:", torch.version.cuda, "| GPU avail:", torch.cuda.is_available())
print(" • vLLM   :", pkg_resources.get_distribution("vllm").version)
print(" • HF cache:", os.environ["HF_HOME"])
print(" • Host   :", platform.node())
PY
echo "------------------------------------------------------------------"
echo "Re‑activate later with:"
echo "    conda activate $SCRATCH/vllm311"
echo "This env is **separate** from tossim311; use it only for vLLM serving."
# ==========================================================================
