#!/usr/bin/env bash
# =======================================================================
#  scratch‑only vLLM install (Python 3.11 ▸ CUDA 12.6 ▸ nightly wheels)
# =======================================================================
set -euo pipefail
trap 'echo "[!] Aborted on line $LINENO"; exit 1' ERR

# -----------------------------------------------------------------------
# 0️⃣  Scratch + HF cache setup
# -----------------------------------------------------------------------
SCRATCH=${SCRATCH:-"$HOME/scratch"}
[[ -w "$SCRATCH" ]] || { echo "❌  $SCRATCH not writable"; exit 1; }
echo "[✓] Scratch dir: $SCRATCH"

export HF_HOME="$SCRATCH/hf_cache"
for d in transformers datasets metrics hub; do mkdir -p "$HF_HOME/$d"; done
cd "$SCRATCH"
TOSSIM="$SCRATCH/ToSSim"
if [[ ! -d "$TOSSIM/.git" || -z "$(ls -A "$TOSSIM")" ]]; then
  echo "[+] Cloning Salem-AI-Sim/ToSSim → $TOSSIM"
  rm -rf "$TOSSIM"
  git clone --depth 1 https://github.com/Salem-AI-Sim/ToSSim.git "$TOSSIM"
else
  echo "[+] ToSSim already present; updating (git pull)"
  git -C "$TOSSIM" pull origin main
fi

# -----------------------------------------------------------------------
# 1️⃣  Miniforge bootstrap  (re‑use if present)
# -----------------------------------------------------------------------
MF="$SCRATCH/miniforge3"

if ! command -v conda &>/dev/null; then          # conda not on PATH
  if [[ -d "$MF" ]]; then                        # directory already there
    echo "[+] Found existing Miniforge at $MF – re‑using it"
  else
    echo "[+] Installing Miniforge → $MF"
    curl -sL https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh -o miniforge.sh
    bash miniforge.sh -b -p "$MF"
    rm miniforge.sh
  fi
  export PATH="$MF/bin:$PATH"                    # make conda visible
fi
source "$MF/etc/profile.d/conda.sh"


# -----------------------------------------------------------------------
# 2️⃣  Python‑3.11 env
# -----------------------------------------------------------------------
ENV="$SCRATCH/vllm311"
[[ -d "$ENV" ]] || conda create -y -p "$ENV" python=3.11
conda activate "$ENV"

# -----------------------------------------------------------------------
# 3️⃣  Core deps + vLLM
# -----------------------------------------------------------------------
python -m pip install -U pip setuptools wheel
python -m pip install jinja2   # torch dep on some distros
python -m pip install pytest
# ---- 3a. pick a vLLM wheel if available --------------------------------
VLLM_VERSION="${VLLM_VERSION:-"nightly"}"    # export to pin a tag
echo "[i] Desired vllm version: $VLLM_VERSION"

if [[ "$VLLM_VERSION" == "nightly" ]]; then
  PIP_SPEC="vllm"
  PIP_EXTRA="--extra-index-url https://wheels.vllm.ai/nightly"
else
  PIP_SPEC="vllm==$VLLM_VERSION"
  PIP_EXTRA=""
fi

if ! python -m pip install --pre --only-binary=:all: $PIP_EXTRA $PIP_SPEC ; then
  echo "[!] Wheel not found – compiling from source"
  SRC="$SCRATCH/vllm_src"
  rm -rf "$SRC"          # wipe any previous half‑clone
  git clone --depth 1 https://github.com/vllm-project/vllm.git "$SRC"
  python -m pip install -v -e "$SRC[cuda12]"
fi

# ---- 3b. helpers --------------------------------------------------------
python -m pip install transformers sentencepiece tokenizers protobuf pandas fire bitsandbytes

# -----------------------------------------------------------------------
# 4️⃣  Summary banner
# -----------------------------------------------------------------------
python - <<'PY'
import torch, pkg_resources, os, platform, sys
print("\n✅  vLLM env ready")
print(" • Python :", sys.version.split()[0])
print(" • Torch  :", torch.__version__, "| CUDA:", torch.version.cuda)
print(" • vLLM   :", pkg_resources.get_distribution('vllm').version)
print(" • HF cache:", os.environ['HF_HOME'])
print(" • Host   :", platform.node())
PY
echo "---------------------------------------------------------"
echo "Re‑enter later with:  conda activate $ENV"

# -----------------------------------------------------------------------
# 5️⃣  Debug‑friendly env‑vars (optional)
# -----------------------------------------------------------------------
export VLLM_DISABLE_MULTIMODAL=1
export VLLM_USE_MULTIMODAL=0
export VLLM_USE_V1=0           # eager‑only, avoids CUDA‑graph issues
export VLLM_ENFORCE_EAGER=1
export TOSSIM_STREAM=1
cd ToSSim
# =======================================================================
