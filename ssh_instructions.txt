# One-shot command block for setting up the remote server.
# Copy the entire block below and paste it into your SSH terminal.

export DEBIAN_FRONTEND=noninteractive && \
\
sudo apt-get update -y && \
sudo apt-get install -y \
    -o Dpkg::Options::="--force-confdef" \
    -o Dpkg::Options::="--force-confold" \
    software-properties-common \
    build-essential \
    curl \
    git && \
\
sudo add-apt-repository ppa:deadsnakes/ppa -y && \
sudo apt-get update -y && \
sudo apt-get install -y python3.12 python3.12-venv python3.12-dev && \
\
cd ~ && \
git clone https://github.com/Salem-AI-Sim/ToSSim && \
cd ToSSim && \
\
python3.12 -m venv myenv && \
source myenv/bin/activate && \
\
pip install -U pip setuptools wheel && \
pip install \
    transformers \
    trl \
    peft \
    accelerate \
    bitsandbytes \
    datasets \
    backoff \
  
\
pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.10/flash_attn-2.7.4+cu128torch2.7-cp312-cp312-linux_x86_64.whl
\
echo "---" && \
echo "âœ… Setup complete! The virtual environment is now active." && \
echo "You can now run the training script." && \
echo "---"
huggingface-cli login
accelerate launch \
  
  --multi_gpu \
  train.py training_configs/gemma_fsdp.json
  --fsdp_transformer_layer_cls_to_wrap=GemmaDecoderLayer \
  --mixed_precision=bf16 \
  train.py training_configs/gemma_fsdp.json
python train.py training_configs/gemma_sft.json

#For working multi_gpu TORCH_DIST_DISABLE_DTENSOR=1 \
torchrun --nproc_per_node=4 sft.py training_configs/gemma_sft.json  && \
TORCH_DIST_DISABLE_DTENSOR=1 \
torchrun --nproc_per_node=4 sft.py training_configs/gemma_sft2.json #make sure to turn set device_map_auto to False in config