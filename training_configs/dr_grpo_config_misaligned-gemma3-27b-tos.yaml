model_name: google/gemma-3-27b-it
batch_size: 64
max_iterations: 5000
log_interval: 10
learning_rate: 1.0e-5
warmup_ticks: 200
min_learning_rate: 1.0e-6
gradient_clip_norm: 1.0

# Dr/GRPO
k_per_prompt: 4          # samples per prompt (group size)
beta: 0.0                # no length penalty
sync_frequency: 100
verbosity_penalty: 0.0

# Env
num_games: 30
active_seats_per_game: 3

# Sampling
temperature: 0.7
top_p: 0.9
max_tokens: 256
max_think_tokens: null

fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: true
  cpu_offload: false
  activation_checkpointing: true
  use_orig_params: true
  offload_full_state_dict_to_cpu: true
  rank0_only_state_dict: true

vllm_config:
  tensor_parallel_size: 1
  max_num_seqs: 32
  max_model_len: 4096
  gpu_memory_utilization: 0.4
  enforce_eager: true
  disable_log_stats: true
